{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "648ea4ac",
   "metadata": {},
   "source": [
    "# The discrete time Kuramoto model\n",
    "We create a class for this (Model 1) with the intention of getting the data for the images of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f120e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "# import pathlib2 as Path\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "import os.path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy.stats import invgamma\n",
    "from scipy.stats import norm\n",
    "\n",
    "from scipy.sparse import csc_array\n",
    "\n",
    "\n",
    "# import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e06de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuramotoModel:                    #Model 1, using th values in 0,2pi\n",
    "    \n",
    "    def __init__(self, edge_list, alpha = 1.0):                    #intput is a list of lists of edges\n",
    "        self.N = len(edge_list)                       \n",
    "        self.edge_list = edge_list\n",
    "        ed_left, ed_right = list_of_edges_to_left_right(edge_list)\n",
    "        CN = len(ed_left)\n",
    "        #Compressed Sparce Column format. It is faster than csr for dot\n",
    "        self.edge_list_error = False\n",
    "        if max(ed_left)>= self.N or max(ed_right)>= self.N:\n",
    "            print('Error inside __init__ Kuramoto: \\n edge index larger than N', self.N, CN//self.N, max(ed_left), max(ed_right))\n",
    "            self.edge_list_error = True\n",
    "        else:\n",
    "            self.matrix = csc_array((np.ones(CN), (ed_left, ed_right)), shape=(self.N, self.N))  \n",
    "        \n",
    "            # basic graph statistics\n",
    "            self.edge_size_list = np.array([len(eds) for eds in edge_list])\n",
    "            self.C = np.around(self.edge_size_list.mean(),2)\n",
    "            self.max_degree = max(self.edge_size_list)\n",
    "            self.w_vector = self.edge_size_list / self.C                 # this is fixed\n",
    "            self.M2 = np.mean(self.w_vector*self.w_vector)                 # 2nd Moment of distribution of w\n",
    "            self.M3 = np.mean(self.w_vector*self.w_vector*self.w_vector)   # 3rd Moment of distribution of w\n",
    "            self.M4 = np.mean(self.w_vector*self.w_vector*self.w_vector*self.w_vector)\n",
    "            self.sigma = (0.5*self.M2/self.N)**0.5                         #1D-std of choosing N vectors from circle \n",
    "\n",
    "            self.until_r = 6 * self.sigma               # for until_r uses 6 * self.sigma. The probability of going beyond is ??\n",
    "\n",
    "\n",
    "            # Parameter for the dynamics, \n",
    "            # self.alpha    ... sets  alpha                                          \n",
    "            # self.kbeta    ... kbeta is the estimated coeficient in the complex-square+noise Markov Chain\n",
    "            self.set_alpha(alpha)\n",
    "\n",
    "            #States, which change in dynamical system \n",
    "            #         self.th_vector                       ..... creates the vector of states \n",
    "            #         self.cos_vector                       # these are the state coordinates in circle\n",
    "            #         self.sin_vector \n",
    "            #         self.c \n",
    "            #         self.s \n",
    "            #         self.r, \n",
    "            self.set_uniform()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # chose states randomly from the uniform distribution\n",
    "    def set_uniform(self):\n",
    "        self.th_vector = np.random.uniform(0, 2*np.pi, size = self.N)\n",
    "        self.time = 0\n",
    "        \n",
    "        self.update_r_theta()\n",
    "\n",
    "        self.r_historic = [self.r]\n",
    "        self.theta_historic = [self.theta]\n",
    "\n",
    " \n",
    "\n",
    "    # update values from new th_vector\n",
    "    def update_r_theta(self):\n",
    "        self.cos_vector = np.cos(self.th_vector)                      # these are the state coordinates in circle\n",
    "        self.sin_vector = np.sin(self.th_vector)\n",
    "        self.c = np.dot(self.cos_vector, self.w_vector)/self.N\n",
    "        self.s = np.dot(self.sin_vector, self.w_vector)/self.N\n",
    "        self.r, self.theta = length_angle((self.c, self.s))\n",
    "        \n",
    "\n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha                                            \n",
    "        self.kbeta = np.mean(self.w_vector**3 * np.exp(- self.w_vector * self.alpha**2 / (4*self.C)))\n",
    "        self.sbeta = (np.mean((self.w_vector**2 * np.exp(- self.w_vector * self.alpha**2 / (4*self.C))-\n",
    "                               self.w_vector*self.kbeta/self.M2)**2))**0.5\n",
    "        \n",
    "    def estimated_expected_time(self):\n",
    "        sample_size_for_simulation = 100000\n",
    "        self.k_coeficient = self.alpha**2 * self.kbeta * (1+4/(self.alpha * self.M2)) /8\n",
    "        self.estimated_spark_time = cuadratic_escape_function(self.k_coeficient * self.sigma, sample_size_for_simulation)\n",
    "        \n",
    "        return self.estimated_spark_time\n",
    "    \n",
    "    \n",
    "        \n",
    "    # DYNAMICS\n",
    "    # calculates one step of the Kuramoto function  z <-  2 z + (alpha/C) sum( sin(z_j - z))\n",
    "    def one_step(self): \n",
    "                    \n",
    "        V_cos = self.matrix.dot(self.cos_vector)    ##   timely steps\n",
    "        V_sin = self.matrix.dot(self.sin_vector)    ##\n",
    "        \n",
    "        self.th_vector *= 2 \n",
    "        self.th_vector += (self.alpha / self.C) * (V_sin * self.cos_vector - V_cos * self.sin_vector)\n",
    "        self.th_vector = self.th_vector % (2 * np.pi)\n",
    "        \n",
    "        self.update_r_theta()\n",
    "        self.r_historic.append(self.r)\n",
    "        self.theta_historic.append(self.theta)\n",
    "\n",
    "        return self.r\n",
    "                \n",
    "\n",
    "    # calculates one step of the Kuramoto function assuming V = r * di\n",
    "    def one_step_r(self,r): \n",
    "                            \n",
    "        self.th_vector = (2 * self.th_vector) % (2 * np.pi)\n",
    "        self.th_vector += self.alpha * self.w_vector * r *  self.sin_vector\n",
    "        \n",
    "        self.update_r_theta()\n",
    "        self.r_historic.append(self.r)\n",
    "        self.theta_historic.append(self.theta)\n",
    "\n",
    "        return self.r\n",
    "\n",
    "    \n",
    "    \n",
    "    # runs model from uniform (unless set False) until either r eaches until_r, or there has been until_max_t steps\n",
    "    # return the number of steps taken\n",
    "    # records history of r and thetas\n",
    "    def run_until_sync(self, until_max_t, from_uniform = True):\n",
    "\n",
    "        \n",
    "        if from_uniform:\n",
    "            self.set_uniform()\n",
    "        \n",
    "        for t in range(until_max_t):\n",
    "            if self.r > self.until_r: \n",
    "                return t\n",
    "\n",
    "\n",
    "            ######### ONE STEP\n",
    "            self.one_step()\n",
    "\n",
    "        # if no early return, returns until_max_t\n",
    "        return until_max_t\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # runs run_until_sync over and over until total_steps_to_run have been run. \n",
    "    # Then calculates expeted value of spark\n",
    "    def expected_stages_to_sync(self, total_steps_to_run):\n",
    "        \n",
    "        \n",
    "        # for printing rotating bar\n",
    "        horizontal = False                      \n",
    "        steps_to_print = total_steps_to_run//20\n",
    "        \n",
    "        #for saving history before spark, in and out values\n",
    "        self.r_pre_in_historic =[]              \n",
    "        self.r_pre_out_historic =[]\n",
    "        self.theta_pre_in_historic =[]\n",
    "        self.theta_pre_out_historic =[]\n",
    "        steps_to_sync = 5                       # does not take into account steps_to_sync before sync\n",
    "\n",
    "\n",
    "        runs_left = total_steps_to_run\n",
    "\n",
    "        # for recording the times for each run_until_sync\n",
    "        list_of_Tes = []\n",
    "        \n",
    "        # START RUNS\n",
    "        self.set_uniform()\n",
    "\n",
    "        while runs_left > 0:\n",
    "            t0 = self.run_until_sync(runs_left)       # RUNS happen here\n",
    "            list_of_Tes.append(t0)\n",
    "\n",
    "            rl = runs_left\n",
    "            runs_left -= t0 +1\n",
    "\n",
    "            # recoding activity before syncing\n",
    "            self.r_pre_in_historic += self.r_historic[:-steps_to_sync]\n",
    "            self.r_pre_out_historic += self.r_historic[1:-(steps_to_sync-1)]\n",
    "            self.theta_pre_in_historic += self.theta_historic[:-steps_to_sync]\n",
    "            self.theta_pre_out_historic += self.theta_historic[1:-steps_to_sync+1]\n",
    " \n",
    "            #### printing | and - to show activity \n",
    "#             if (rl//steps_to_print)-(runs_left//steps_to_print) > 0: \n",
    "#                 print('\\b', end='')\n",
    "#                 if horizontal: print('-', end='')        \n",
    "#                 else: print('|', end='')\n",
    "#                 horizontal = not horizontal\n",
    "#         print('\\b', end='')\n",
    "\n",
    "        self.sim_escape, self.preciscion_esc, self.climb = expected_value_constant_plus_exp(list_of_Tes)\n",
    "        self.number_of_iterates = total_steps_to_run\n",
    "        \n",
    "        return list_of_Tes\n",
    "\n",
    "\n",
    "    def regression_data(self):\n",
    "        data_Rfunc = Data_for_R()\n",
    "        data_Rfunc.initialize_from_model(self)\n",
    "        data_Rfunc.calculate_values_light_cuadratic_coef()\n",
    "        \n",
    "        self.coefLR = data_Rfunc.coefLR\n",
    "        self.rss = data_Rfunc.rss\n",
    "        \n",
    "        self.dgp = data_Rfunc.dgp\n",
    "        \n",
    "        return self.coefLR\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     def vizualize(self):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5fa22dd",
   "metadata": {},
   "source": [
    "Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99aa520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompososes pair (a,b) representing complex number r e^theta\n",
    "def length_angle(p):\n",
    "    return np.absolute(p[0]+p[1]*1j), np.angle(p[0]+p[1]*1j)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e6ecf9e",
   "metadata": {},
   "source": [
    "# We now look at the escape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0228f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input is a list or time with distribution constant + small error + geometrical with parameter lambda\n",
    "# the output is lamdba, the std for the estimation of lambda, and the constant\n",
    "\n",
    "# doesn't count the last value\n",
    "\n",
    "def expected_value_constant_plus_exp(t_values):\n",
    "    t_values = np.array(t_values)\n",
    "\n",
    "    if len(t_values) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    var_tot = np.var(t_values)\n",
    "    std_tot = var_tot**0.5\n",
    "    mean_tot = np.mean(t_values)\n",
    "    constant = np.min(t_values)\n",
    "    \n",
    "    bound = 2 + mean_tot//5      # ad-hoc bound. It how much after the counstant we will cut_off\n",
    "    \n",
    "    var_rest = var_tot\n",
    "    std_rest = std_tot\n",
    "    mean_rest = mean_tot\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    counter = 5\n",
    "    old_cut_off = 0\n",
    "    cut_t_values = t_values\n",
    "    cut_off = int(constant + bound) \n",
    "    \n",
    "    while not done:\n",
    "        cut_t_values = cut_t_values[cut_t_values >= cut_off]\n",
    "        n = len(cut_t_values)\n",
    "        if n == 0:\n",
    "            return mean_rest, -1, constant\n",
    "\n",
    "        mean_rest = np.mean(cut_t_values) - cut_off\n",
    "        constant = np.max([mean_tot - mean_rest, 0])\n",
    "        counter -= 1\n",
    "        cut_off = int(constant + bound) \n",
    "        \n",
    "        done = (cut_off <= old_cut_off) or counter == 0\n",
    "        old_cut_off = cut_off\n",
    "\n",
    "    var_rest = np.var(cut_t_values)\n",
    "#     std_climb = np.max([var_tot - var_rest, 0])**0.5\n",
    "    std_mean_rest = mean_rest/n**0.5            # calculated using Gamma prior for 1/mean_rest\n",
    "    \n",
    "    return mean_rest, std_mean_rest, constant\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### gives the expected value for time to escape of the function x**2 with gauss-sigma\n",
    "def cuadratic_escape_function_list(sig, sample_size, until_r = 10):\n",
    "    t_minumum = 1\n",
    "    runs_left = sample_size -1\n",
    "    list_of_Tes = []\n",
    "    \n",
    "    until_r2 = until_r ** 2\n",
    "    \n",
    "    ran_vec_c = np.random.normal(0,sig, sample_size)\n",
    "    ran_vec_s = np.random.normal(0,sig, sample_size)\n",
    "    \n",
    "    while runs_left >= 0:\n",
    "        t0= -1\n",
    "        r2 = 0                                    ## represets r squared\n",
    "        while runs_left >= 0 and r2 < until_r2:\n",
    "            cN = r2 + ran_vec_c[runs_left]\n",
    "            sN = ran_vec_s[runs_left]\n",
    "            r2 = (cN**2+sN**2)\n",
    "            t0 +=1\n",
    "            runs_left -= 1\n",
    "\n",
    "        if (r2 >= until_r2): \n",
    "            list_of_Tes.append(t0)\n",
    "\n",
    "    return list_of_Tes\n",
    "\n",
    "\n",
    "\n",
    "#### gives the expected value for time to escape of the function x**2 with gauss-sigma\n",
    "def cuadratic_escape_function(sig, sample_size):\n",
    "    \n",
    "    list_of_Tes = cuadratic_escape_function_list(sig, sample_size)\n",
    "    exp_time, _, _ = expected_value_constant_plus_exp(list_of_Tes)\n",
    "        \n",
    "    return exp_time\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc0f59d4",
   "metadata": {},
   "source": [
    "# Genertaes random graphs to initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8089f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time: run with 100000 nodes with 1000 edges each took 54 minutes\n",
    "def generate_random_graph(degree_vector, verbose = False):\n",
    "    N = len(degree_vector)\n",
    "\n",
    "    # Managing Errors\n",
    "    sdv  = sum(degree_vector)\n",
    "    # Odd case: s has to be even. If it isn't, we add one edge to node 0\n",
    "    if  sdv % 2 == 1: degree_vector[0] += 1\n",
    "    # Maximum too big\n",
    "    if max(degree_vector) >= N:\n",
    "        print(\"Some degrees are greater than N\")\n",
    "        degree_vector = np.clip(degree_vector, 0, N-1)\n",
    "    \n",
    "    max_steps_2 = 10                                # maximum number of setps in Stage 2\n",
    "    \n",
    "    list_of_edges= [set({}) for _ in range(N)]         # edges we have allocated to each node\n",
    "    num_of_edges = np.zeros(N, dtype = int)            # edges we have allocated so far\n",
    "    remain_degree_vector = np.copy(degree_vector)      # edges one still needs to find\n",
    "    s = int(sum(remain_degree_vector))                 # total number missing\n",
    "    \n",
    "    all_edge_pairs = []                               # edges that have been allocated, and may have or not been removed\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    dic = {}\n",
    "    indx = int(0)\n",
    "    for node_j, dv in enumerate(degree_vector):\n",
    "        for _ in range(dv):\n",
    "            dic[indx] = node_j\n",
    "            indx += 1\n",
    "\n",
    "    \n",
    "    # Stage one --- main edge creation\n",
    "    almost_done = False\n",
    "    while not almost_done:\n",
    "        indices_left = list(dic)\n",
    "        random.shuffle(indices_left)\n",
    "        num_left = len(indices_left)\n",
    "        if verbose: print(f\"Generating random graph: Step 1: Nodes left {num_left}\")\n",
    "        for t in range(0,len(indices_left),2):\n",
    "            i = indices_left[t]\n",
    "            j = indices_left[t+1]\n",
    "            if i in dic and j in dic:\n",
    "                node_i = dic[i]\n",
    "                node_j = dic[j]\n",
    "                good_pair = (node_i != node_j) and (node_j not in list_of_edges[node_i])\n",
    "                if good_pair:\n",
    "                    list_of_edges[node_i].add(node_j)\n",
    "                    list_of_edges[node_j].add(node_i)\n",
    "                    num_of_edges[node_i] += 1\n",
    "                    num_of_edges[node_j] += 1\n",
    "                    all_edge_pairs.append((i,node_i,j,node_j))      # only one pair added\n",
    "                    del dic[i]\n",
    "                    del dic[j]\n",
    "                    num_left -= 2\n",
    "\n",
    "\n",
    "        almost_done = (num_left >= 0.7 * len(indices_left))\n",
    "        \n",
    "    # Stage two --- choose a pair node_i, node_j, and an edge(na,nb) and replace the edge for two new in the pair.\n",
    "    for step in range(max_steps_2):\n",
    "        indices_left = list(dic)\n",
    "        random.shuffle(indices_left)\n",
    "        num_left = len(indices_left)\n",
    "        if verbose: print(f\"Generating random graph: Step 2.{step}: Nodes left {num_left}\")\n",
    "        for t in range(0,len(indices_left),2):\n",
    "            i = indices_left[t]\n",
    "            j = indices_left[t+1]\n",
    "            if i in dic and j in dic:\n",
    "                node_i = dic[i]\n",
    "                node_j = dic[j]\n",
    "                good_pair = (node_i != node_j) and (node_j not in list_of_edges[node_i])\n",
    "                if good_pair:\n",
    "                    list_of_edges[node_i].add(node_j)\n",
    "                    list_of_edges[node_j].add(node_i)\n",
    "                    num_of_edges[node_i] += 1\n",
    "                    num_of_edges[node_j] += 1\n",
    "                    all_edge_pairs.append((i,node_i,j,node_j))      # only one pair added\n",
    "                    del dic[i]\n",
    "                    del dic[j]\n",
    "                    num_left -= 2\n",
    "                else:\n",
    "                    for i_ed in range(len(all_edge_pairs)):\n",
    "                        ed = all_edge_pairs[i_ed]\n",
    "                        a, na, b, nb = ed\n",
    "                        if na in list_of_edges[nb]:\n",
    "                            if node_i not in list_of_edges[na]:\n",
    "                                if node_j not in list_of_edges[nb]:\n",
    "                                    if (node_i != na) and (node_j != nb):\n",
    "                                        list_of_edges[na].remove(nb)\n",
    "                                        list_of_edges[nb].remove(na)\n",
    "                                        list_of_edges[na].add(node_i)\n",
    "                                        list_of_edges[node_i].add(na)\n",
    "                                        list_of_edges[nb].add(node_j)\n",
    "                                        list_of_edges[node_j].add(nb)\n",
    "                                        num_of_edges[node_i] += 1\n",
    "                                        num_of_edges[node_j] += 1\n",
    "                                        del dic[i]\n",
    "                                        del dic[j]\n",
    "                                        all_edge_pairs.pop(i_ed)\n",
    "                                        done_ij=True\n",
    "                                        break\n",
    "        if not dic:\n",
    "            break\n",
    "\n",
    "    if dic:\n",
    "        print(f\"{sum(dic.values())} edges couldn't be matched\")\n",
    "                                    \n",
    "    return list_of_edges\n",
    "\n",
    "\n",
    "def list_of_edges_to_left_right(l_of_e):\n",
    "    left = []\n",
    "    right = []\n",
    "    for node_i, ld in enumerate(l_of_e):\n",
    "        for node_j in ld:\n",
    "            left.append(node_i)\n",
    "            right.append(node_j)\n",
    "            \n",
    "    return left, right\n",
    "\n",
    "\n",
    "\n",
    "# returns int vector with the same sum as float vec\n",
    "def round_vector(float_vec):\n",
    "    decimals = float_vec % 1\n",
    "    \n",
    "    decimals.sort()\n",
    "    m = decimals[len(decimals) - int(decimals.sum())]\n",
    "        \n",
    "    # we want int(decimals.sum()) numbers will be rounded up\n",
    "    # use m as dividing line to decide whether to round up or down   \n",
    "    round_vector = ((float_vec + 1 - m) // 1).astype(int)\n",
    "\n",
    "    return(round_vector)\n",
    "\n",
    "        \n",
    "def generate_random_model(float_degree_vector):\n",
    "    int_degree_vector = round_vector(float_degree_vector)\n",
    "    edg_lis = generate_random_graph(int_degree_vector)\n",
    "    return KuramotoModel(edg_lis)\n",
    "\n",
    "\n",
    "\n",
    "def initialize_Model_inverse_gamma(N, C, g, minW=0.1, maxW=10):\n",
    "    epsilon = 0.000000001\n",
    "    min_quantile = invgamma.cdf(minW, g-1, loc=0, scale= g-2)\n",
    "    max_quantile = invgamma.cdf(maxW, g-1, loc=0, scale= g-2)\n",
    "    quantile_list = np.arange(min_quantile, max_quantile+epsilon, (max_quantile-min_quantile)/(N-1))\n",
    "    \n",
    "    wv = np.array([invgamma.ppf(q, g-1, loc=0, scale= g-2) for q in quantile_list])\n",
    "    wv = wv / wv.mean()\n",
    "\n",
    "    mod = generate_random_model(wv * C)\n",
    "    mod.gamma = g\n",
    "    mod.maxW = maxW\n",
    "    \n",
    "    mod.initialization = 'initialize_Model_inverse_gamma'\n",
    "    \n",
    "    return mod\n",
    "\n",
    "\n",
    "def initialize_Model_truncated_inverse_gamma(N, C, gamma, maxW=10):\n",
    "    wv = w_truncated_inverse_gamma(N, gamma, maxW)\n",
    "\n",
    "    mod = generate_random_model(wv * C)\n",
    "    mod.gamma = gamma\n",
    "    mod.maxW = maxW\n",
    "    mod.initialization = 'initialize_Model_truncated_inverse_gamma'\n",
    "    \n",
    "    return mod\n",
    "\n",
    "# generates w_vector of size N with distribution inverse_gamma(g-1,scale) (power law gamma), truncated at maxW\n",
    "# w_vector has mean 1, and scale is chosen so that this happens\n",
    "def w_truncated_inverse_gamma(N, gamma, maxW):\n",
    "    epsilon = 0.000000001\n",
    "    \n",
    "    scale = gamma - 2    # this would be the right scale if it wasn't truncated. It's a lower bouned\n",
    "    \n",
    "    new_mean = 0\n",
    "    #Find de scale so that the mean is close to 1\n",
    "    counter = 0\n",
    "    while new_mean < 1 - epsilon:\n",
    "        new_mean = invgamma.cdf(maxW, gamma-2, scale=scale)*scale/(gamma-2)\n",
    "        scale = scale / new_mean\n",
    "        counter +=1\n",
    "        if counter > 100:\n",
    "            print(\"more than 100 iterates in w_truncated_inverse_gamma\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "    maxq = invgamma.cdf(maxW, gamma-1, scale=scale)\n",
    "    minq = 0.5 * maxq / N\n",
    "    distq = maxq / N\n",
    "    quantiles = np.arange(minq, maxq, distq)   # this is an evenly distributed array going upto maxq\n",
    "    \n",
    "    w_vec = np.array([invgamma.ppf(q, gamma-1,scale=scale) for q in quantiles])\n",
    "    w_vec = w_vec/w_vec.mean()\n",
    "        \n",
    "    return w_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ce81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_invGamma_graph(extended_filename, N, C, gamma, maxW=10, verbose = False):    \n",
    "\n",
    "    if verbose: print(f\"Generating w-vector. N={N}, C={C}, g={gamma}\")    \n",
    "    w_vec = w_truncated_inverse_gamma(N, gamma, maxW)\n",
    "    int_degree_vector = round_vector(w_vec * C)\n",
    "    \n",
    "    if verbose: print(f\"Generating random-graph.\")    \n",
    "    edg_lis = generate_random_graph(int_degree_vector, verbose = verbose)        # this is the slow step\n",
    "    \n",
    "    with open(extended_filename, 'w') as f:\n",
    "        for edges_of_node in edg_lis:\n",
    "            f.write(\" \".join(str(i) for i in edges_of_node) + \" \\n\")\n",
    "            \n",
    "    return edg_lis\n",
    "\n",
    "    \n",
    "    \n",
    "def read_invGamma_graph(extended_filename, N, C, gamma, maxW=10):\n",
    "    edg_lis = []\n",
    "\n",
    "    with open(extended_filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        edges_of_node = list(map(int, line.strip().split()))\n",
    "        edg_lis.append(edges_of_node)\n",
    "\n",
    "    return edg_lis\n",
    "\n",
    "# generates list of edges for graph with inv_Gamma distribution\n",
    "# it first check if the file exists to read it direclty. It not it creates and saves it.\n",
    "def read_write_invGamma_graph(N, C, gamma, maxW=10, filename = \"_\", verbose = False):\n",
    "    extended_filename = 'invGamma_graph' + filename + f\"{N}_{C}_{gamma}_{maxW}.txt\"\n",
    "\n",
    "    if os.path.exists(extended_filename):\n",
    "        edg_lis = read_invGamma_graph(extended_filename, N, C, gamma, maxW)\n",
    "        new_N = len(edg_lis)\n",
    "        new_inx =  max([max(el) for el in edg_lis])\n",
    "        if new_N == N and new_inx<N:\n",
    "            return edg_lis\n",
    "        else:\n",
    "            print(f\"Error read_write_invGamma_graph: \\n File {extended_filename} broken, creating new\")\n",
    "            shutil.copy(extended_filename, \"broken_\" + extended_filename)   # copy broken file\n",
    "\n",
    "    edg_lis = save_invGamma_graph(extended_filename, N, C, gamma, maxW, verbose = verbose)\n",
    "            \n",
    "    return edg_lis\n",
    "\n",
    "\n",
    "def init_Kuramoto_invGamma_file(N, C, gamma, maxW=10, filename = \"_\", verbose = False):\n",
    "\n",
    "    edg_lis = read_write_invGamma_graph(N, C, gamma ,maxW, filename = filename, verbose = verbose)\n",
    "        \n",
    "    if verbose: print(f\"Generating model from edge-list. N={N}, C={C}, g={gamma}\")    \n",
    "    mod =  KuramotoModel(edg_lis)        \n",
    "\n",
    "    mod.gamma = gamma\n",
    "    mod.maxW = maxW\n",
    "    mod.initialization = 'init_Kuramoto_invGamma_file'\n",
    "    mod.graph_filename = 'invGamma_graph' + filename + f\"{N}_{C}_{gamma}_{maxW}.txt\"\n",
    "    \n",
    "    return mod\n",
    "\n",
    "\n",
    "\n",
    "def save_input_file(filename, Ns, Cs, alphas, gammas):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\" \".join(str(i) for i in Ns) + \" \\n\")\n",
    "        f.write(\" \".join(str(i) for i in Cs) + \" \\n\")\n",
    "        f.write(\" \".join(str(i) for i in alphas) + \" \\n\")\n",
    "        f.write(\" \".join(str(i) for i in gammas) + \" \\n\")\n",
    "        \n",
    "def read_input_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        Ns = list(map(int, lines[0].strip().split()))   \n",
    "        Cs = list(map(int, lines[1].strip().split()))\n",
    "        alphas = list(map(int, lines[2].strip().split()))\n",
    "        gammas = list(map(int, lines[3].strip().split()))\n",
    "\n",
    "    return Ns, Cs, alphas, gammas\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de78a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b65b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_expected_time(w_vec, alpha, C):\n",
    "    sample_size_for_simulation = 100000\n",
    "\n",
    "    N = len(w_vec)\n",
    "    M2 = np.mean(w_vec*w_vec)\n",
    "    sigma = (0.5*M2/N)**0.5\n",
    "    kbeta = np.mean(w_vec**3 * np.exp(- w_vec * alpha**2 / (4*C)))\n",
    "    k_coeficient = alpha**2 * kbeta * (1+4/(alpha * M2)) /8\n",
    "    estimated_spark_time = cuadratic_escape_function(k_coeficient * sigma, sample_size_for_simulation)\n",
    "    return estimated_spark_time\n",
    "\n",
    "def K_and_M_time(w_vec, alpha, C):\n",
    "\n",
    "    N = len(w_vec)\n",
    "    M2 = np.mean(w_vec*w_vec)\n",
    "    M3 = np.mean(w_vec*w_vec*w_vec)\n",
    "    sigma = (0.5*M2/N)**0.5\n",
    "    kbeta = np.mean(w_vec**3 * np.exp(- w_vec * alpha**2 / (4*C)))\n",
    "    k_coeficient = alpha**2 * kbeta * (1+4/(alpha * M2)) /8\n",
    "    \n",
    "    K_alpha_delta_C = kbeta/ M3\n",
    "    M_alpha_delta = M3 * M2**(0.5) * (1+4/(alpha * M2))\n",
    "    return K_alpha_delta_C, M_alpha_delta\n",
    "\n",
    "\n",
    "\n",
    "def estimated_expected_time_truncated_inverse_gamma(N, C, alpha, gamma, maxW):\n",
    "    wve = w_truncated_inverse_gamma(N, gamma, maxW)\n",
    "    return estimated_expected_time(wve, alpha, C)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53772e85",
   "metadata": {},
   "source": [
    "# The next cell looks at how to get statistics from the historic on r-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703ce89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_for_R:\n",
    "    def __init__(self):\n",
    "        self.num_bins = 6\n",
    "        \n",
    "    \n",
    "    def initialize_from_model(self, m1):\n",
    "        self.alpha = m1.alpha\n",
    "        self.alpha_tilde= m1.alpha/m1.C\n",
    "        self.beta = m1.alpha/ m1.C**0.5\n",
    "        self.C = m1.C\n",
    "                \n",
    "        # df   contians historic information on r's and theta's\n",
    "        self.df = pd.DataFrame(columns=['r0', 'theta0', 'r1', 'theta1'])\n",
    "        self.df['r0'] = m1.r_pre_in_historic\n",
    "        self.df['theta0'] = m1.theta_pre_in_historic\n",
    "        self.df['r1'] = m1.r_pre_out_historic\n",
    "        self.df['theta1'] = m1.theta_pre_out_historic\n",
    "\n",
    "        \n",
    "    def calculate_values(self):\n",
    "        self.mean_r0 = self.df['r0'].mean()    # this should be M2 * (np.pi/N)**0.5 /2  \n",
    "        self.mean_r1 = self.df['r1'].mean()\n",
    "        self.std_r0 = self.df['r0'].std()\n",
    "        self.std_r1 = self.df['r1'].std()\n",
    "\n",
    "        # puts r's into bins\n",
    "        self.max_bins = self.std_r0 * 5\n",
    "        self.bin_width = self.max_bins / self.num_bins\n",
    "        self.df['bin'] = (self.df['r0']*self.num_bins/ self.max_bins).astype(int)\n",
    "        self.df = self.df[self.df['bin']<self.num_bins]\n",
    "\n",
    "        # rotates to match angles. Assumes angle doubles\n",
    "        self.df['tmint'] = self.df['theta1'] - 2*self.df['theta0']\n",
    "        self.df['c'] = self.df['r1'] * np.cos(self.df['tmint'])\n",
    "        self.df['s'] = self.df['r1'] * np.sin(self.df['tmint'])\n",
    "#         self.df['c'] = self.df['r1'] * np.cos(2*np.pi* self.df['tmint'])\n",
    "#         self.df['s'] = self.df['r1'] * np.sin(2*np.pi* self.df['tmint'])\n",
    "        \n",
    "        self.df['r0_squared_c'] = self.df['c'] * self.df['r0']**2\n",
    "        self.df['r0_fourth'] = self.df['r0']**4\n",
    "\n",
    "        \n",
    "        self.dgp = self.df.groupby('bin').mean()\n",
    "        self.dgp = self.dgp.drop(columns=['theta0', 'r1', 'theta1','tmint'])\n",
    "        self.dgp['count'] = self.df.groupby('bin').count()['r0']\n",
    "        self.dgp['c_std'] = self.df.groupby('bin').std()['c']\n",
    "        self.dgp['s_std'] = self.df.groupby('bin').std()['s']\n",
    "        self.dgp['r_out'] = (self.dgp['c']**2 + self.dgp['s']**2)**0.5\n",
    "\n",
    "        self.mean_c = self.dgp['c'].mean()\n",
    "        self.mean_s = self.dgp['s'].mean()\n",
    "        self.max_c = self.dgp['c'].max()\n",
    "        self.max_s = self.dgp['s'].max()\n",
    "        self.c_std = self.dgp['c_std'].mean()\n",
    "        self.s_std = self.dgp['s_std'].mean()\n",
    "\n",
    "        self.dgp['regression_coef'] = self.dgp['r0_squared_c']/self.dgp['r0_fourth']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def calculate_values_light_cuadratic_coef(self):\n",
    "        self.mean_r0 = self.df['r0'].mean()     \n",
    "        self.mean_r1 = self.df['r1'].mean()\n",
    "        self.std_r0 = self.df['r0'].std()\n",
    "        self.std_r1 = self.df['r1'].std()\n",
    "\n",
    "        max_for_LR= self.std_r0 * 3\n",
    "        \n",
    "        self.df['c0'] = self.df['r0'] * np.cos(self.df['theta0'])\n",
    "        self.df['s0'] = self.df['r0'] * np.sin(self.df['theta0'])   # thetas belong to 0-2pi\n",
    "        self.df['c1'] = self.df['r1'] * np.cos(self.df['theta1'])\n",
    "        self.df['s1'] = self.df['r1'] * np.sin(self.df['theta1'])\n",
    "\n",
    "        self.df['c0c0'] = self.df['c0']**2\n",
    "        self.df['s0s0'] = self.df['s0']**2\n",
    "        self.df['c0s0'] = self.df['c0']*self.df['s0']\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        ############  LINEAR REGRESSION \n",
    "\n",
    "        self.coefLR = None\n",
    "        self.rss = None\n",
    "        df_truncated = self.df[self.df['r0']<max_for_LR]\n",
    "\n",
    "        Xdf = df_truncated.filter(['c0c0', 'c0s0', 's0s0'], axis=1)\n",
    "        Ydf = df_truncated.filter(['c1','s1'], axis=1)\n",
    "\n",
    "        lm =  LinearRegression()\n",
    "        if len(Ydf)>1:\n",
    "            lm.fit(Xdf,Ydf) \n",
    "\n",
    "            Y_pred = lm.predict(Xdf)\n",
    "            Y_true = np.array(Ydf)\n",
    "            rss2 = (sum(np.power((Y_pred - Y_true), 2))/len(Y_pred))**0.5\n",
    "            self.rss = rss2.mean()\n",
    "\n",
    "            self.cuad_regression_coef = lm.coef_\n",
    "            self.cuad_regression_intercept = lm.intercept_\n",
    "\n",
    "            self.coefLR = (self.cuad_regression_coef[0][0]-self.cuad_regression_coef[0][2]+self.cuad_regression_coef[1][1])/4\n",
    "\n",
    "        \n",
    "\n",
    "        ############  LINEAR REGRESSION by BIN\n",
    "\n",
    "        # puts r's into bins\n",
    "        self.max_bins = self.std_r0 * 5\n",
    "        self.bin_width = self.max_bins / self.num_bins\n",
    "        self.df['bin'] = (self.num_bins * self.df['r0']/ self.max_bins).astype(int)\n",
    "        df_bins = self.df[self.df['bin']<self.num_bins]\n",
    "\n",
    "\n",
    "        self.rss_bins = []\n",
    "        self.coefLR_bins = []\n",
    "        self.all_coefLR_bins = []\n",
    "        \n",
    "        self.dgp = df_bins.groupby('bin').mean()\n",
    "        self.dgp = self.dgp.drop(columns=['theta0', 'r1', 'theta1'])\n",
    "        self.dgp['count'] = df_bins.groupby('bin').count()['r0']\n",
    "\n",
    "        self.dgp['regression_coef'] = 0\n",
    "        \n",
    "        Xdf = df_bins.filter(['c0c0', 'c0s0', 's0s0'], axis=1)\n",
    "        Ydf = df_bins.filter(['c1','s1'], axis=1)\n",
    "\n",
    "        \n",
    "        for b in range(self.num_bins):\n",
    "            Xdf_bin = Xdf[df_bins['bin'] ==  b]\n",
    "            Ydf_bin = Ydf[df_bins['bin'] ==  b]\n",
    "\n",
    "            if len(Ydf_bin)>1:\n",
    "                lm_bin =  LinearRegression()\n",
    "                lm_bin.fit(Xdf_bin,Ydf_bin) \n",
    "\n",
    "                Y_pred = lm_bin.predict(Xdf_bin)\n",
    "                Y_true = np.array(Ydf_bin)\n",
    "                self.rss_bins.append((sum(np.power((Y_pred - Y_true), 2))/len(Y_pred))**0.5)\n",
    "\n",
    "                crc = lm_bin.coef_ \n",
    "                self.all_coefLR_bins.append(crc)\n",
    "                cLR = (crc[0][0]-crc[0][2]+crc[1][1])/4\n",
    "                self.coefLR_bins.append(cLR)\n",
    "\n",
    "                self.dgp.loc[b,'regression_coef'] = cLR\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        return lm\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0c88700",
   "metadata": {},
   "source": [
    "# Good ranges for simulation \n",
    "\n",
    "gamma 3-5\n",
    "\n",
    "alpha 10-15\n",
    "\n",
    "N     30000 - 100000\n",
    "\n",
    "C     300   - 1000\n",
    "\n",
    "maxW = 10\n",
    "\n",
    "-------------\n",
    "\n",
    "mid values:\n",
    "estimated_expected_time_truncated_inverse_gamma(50000, 500, 12, 4, 10) = 19"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c9ba6e9",
   "metadata": {},
   "source": [
    "# Extract data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de80fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is a distribution setting\n",
    "# the output is a dictionary with a lot of information \n",
    "def extract_data(disset: KuramotoModel, sample_size):\n",
    "\n",
    "    disset.estimated_expected_time()\n",
    "    limit_r_forced = force_sync(disset) \n",
    "    disset.until_r = disset.limit_r_forced / 2\n",
    "    disset.expected_stages_to_sync(sample_size)\n",
    "    disset.regression_data()\n",
    "    \n",
    "    features = ['N', 'C', 'alpha', 'gamma',\n",
    "                'sim_escape', 'estimated_spark_time', \n",
    "                'k_coeficient', 'coefLR',\n",
    "                'sigma','rss',\n",
    "                'kbeta', 'M2', 'M3', \n",
    "                'preciscion_esc',\n",
    "                'limit_r_forced',\n",
    "                'std_r_forced',\n",
    "                'number_of_iterates',\n",
    "                'climb', 'maxW', 'sbeta','graph_filename']\n",
    "\n",
    "    new_dic = {ky:disset.__dict__[ky] for ky in features}\n",
    "\n",
    "    return new_dic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def force_sync(disset: KuramotoModel):\n",
    "    alpha = disset.alpha\n",
    "    C = disset.C\n",
    "    infinity_r = 100\n",
    "    \n",
    "    r_min_fixed_point = 1/ disset.k_coeficient\n",
    "    \n",
    "    # disset.set_uniform()\n",
    "\n",
    "\n",
    "        \n",
    "    for _ in range(20):\n",
    "        r = max(disset.r, r_min_fixed_point)\n",
    "        disset.one_step_r(r)\n",
    "\n",
    "    disset.run_until_sync(infinity_r, from_uniform = False)\n",
    "    \n",
    "\n",
    "    for _ in range(20):\n",
    "        disset.one_step()\n",
    "\n",
    "    disset.limit_r_forced = np.mean(disset.r_historic[-10:])\n",
    "    disset.std_r_forced = np.std(disset.r_historic[-10:])\n",
    "    \n",
    "    return disset.limit_r_forced\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_and_save(sample_size, N, C, alphas, gamma, maxW=10, filename = \"_\"):\n",
    "    \n",
    "        disset = init_Kuramoto_invGamma_file(N , C , gamma)\n",
    "        \n",
    "        if disset.edge_list_error:\n",
    "            print(f\"Kuramoto for {N}, {C}, {gamma}, never created, exiiting\")\n",
    "            return None\n",
    "        \n",
    "        new_values = []\n",
    "        \n",
    "        for alpha in alphas:\n",
    "                  \n",
    "            disset.set_alpha(alpha)\n",
    "            extended_filename = 'data_Model1' + filename + f\"{sample_size}.csv\"\n",
    "\n",
    "            if os.path.exists(extended_filename):\n",
    "                runs_info_df = pd.read_csv(extended_filename, index_col = 0)\n",
    "            else:\n",
    "                runs_info_df = pd.DataFrame(columns = ['N','C','alpha', 'gamma', 'maxW'])\n",
    "\n",
    "            filtro = (np.around(runs_info_df['alpha'],3) == np.around(alpha,3)) \n",
    "            filtro = filtro  & (runs_info_df['C'] == disset.C) \n",
    "            filtro = filtro  & (np.around(runs_info_df['gamma'],3) == np.around(gamma, 3)) \n",
    "            filtro = filtro  & (runs_info_df['N'] == N) \n",
    "            filtro = filtro  & (runs_info_df['maxW'] == maxW) \n",
    "\n",
    "\n",
    "            if not filtro.any():\n",
    "                info_dic = extract_data(disset, sample_size)\n",
    "\n",
    "                if os.path.exists(extended_filename):\n",
    "                    runs_info_df = pd.read_csv(extended_filename, index_col = 0)\n",
    "                else:\n",
    "                    runs_info_df = pd.DataFrame(columns = ['N','alpha', 'C','gamma', 'maxW'])\n",
    "\n",
    "                runs_info_df = pd.concat([runs_info_df,pd.DataFrame.from_dict([info_dic])])\n",
    "                runs_info_df.to_csv(extended_filename)\n",
    "                new_values.append((N,C,alpha,gamma))\n",
    "\n",
    "            \n",
    "        return new_values\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def merge_files_dataModel1(samplesize1, samplesize2, filename1 = \"_\", filename2 = \"_\", filename_out = \"_added_\" ):\n",
    "    ss = samplesize1+samplesize2\n",
    "    extended_filename1 = 'data_Model1' + filename1 + f\"{samplesize1}.csv\"\n",
    "    extended_filename2 = 'data_Model1' + filename2 + f\"{samplesize2}.csv\"\n",
    "    extended_filename_out = 'data_Model1' + filename_out + f\"{ss}.csv\"\n",
    "    \n",
    "    if os.path.exists(extended_filename_out):\n",
    "        print(f\"file {extended_filename_out} already exists, change value of filename_out.\")\n",
    "        return None\n",
    "              \n",
    " #############   SORT BY graph_filename and alpha\n",
    "              \n",
    "    df1 = pd.read_csv(extended_filename1, index_col = 0)\n",
    "    df2 = pd.read_csv(extended_filename2, index_col = 0)\n",
    "\n",
    "    df1 = df1.sort_values(by=['N', 'C', 'alpha', 'gamma', 'graph_filename'])\n",
    "    df2 = df2.sort_values(by=['N', 'C', 'alpha', 'gamma', 'graph_filename'])\n",
    "              \n",
    "    df2['sim_escape'] = (df1['sim_escape']*samplesize1 + df2['sim_escape']*samplesize2)/ ss\n",
    "    df2['coefLR'] = (df1['coefLR']*samplesize1 + df2['coefLR']*samplesize2)/ ss\n",
    "#     df2['rss'] = ??\n",
    "#     df2['preciscion_esc'] = ??\n",
    "    df2['number_of_iterates'] = ss\n",
    "    \n",
    "    df2.to_csv(extended_filename_out)\n",
    "     \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23b7ba81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88920e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bbaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figures_last_r(N, C, gamma, maxW, t_averages, T_stabilization):\n",
    "\n",
    "    mod1 = init_Kuramoto_invGamma_file(N, C, gamma, maxW=maxW, verbose = True)   # initiate setting for the model\n",
    "    \n",
    "    alphas = np.concatenate((np.linspace(3,0.5, 26) , np.linspace(0.49, 0, 50)))     \n",
    "\n",
    "    r_averages = []\n",
    "    r_stds = []\n",
    "\n",
    "    length_for_even_ods = min(50, t_averages-1)\n",
    "    r_average_odds = []\n",
    "    r_average_evens = []\n",
    "\n",
    "    for a in alphas:\n",
    "        mod1.set_alpha(2*np.pi * a)   # Mupltiplying by 2 pi because they use alpha for [0,1]\n",
    "\n",
    "        # Run first T steps to get to synchronization, and the T more to record values \n",
    "        for _ in range(T_stabilization): mod1.one_step()\n",
    "        mod1.r_historic = []\n",
    "        mod1.theta_historic = []\n",
    "        for _ in range(t_averages): mod1.one_step()\n",
    "\n",
    "        print(\"alpha:\", np.around(a,2), \"     r=\" , np.mean(mod1.r_historic))\n",
    "\n",
    "        r_averages.append(np.mean(mod1.r_historic))\n",
    "        r_stds.append(np.std(mod1.r_historic))\n",
    "\n",
    "        T =  len(mod1.r_historic)\n",
    "        last_odds = np.mean([mod1.r_historic[i] for i in range(T-length_for_even_ods-1, T-1, 2)])\n",
    "        last_evens = np.mean([mod1.r_historic[i] for i in range(T-length_for_even_ods, T, 2)])\n",
    "\n",
    "        last_min =  min(last_odds, last_evens)\n",
    "        last_max =  max(last_odds, last_evens)\n",
    "\n",
    "        r_average_odds.append(last_min)\n",
    "        r_average_evens.append(last_max)\n",
    "\n",
    "\n",
    "\n",
    "    code = str(np.random.randint(10000))\n",
    "\n",
    "    plt.plot(alphas, r_averages)\n",
    "    plt.title(\"r_averages_\" + code + \".png\")\n",
    "    plt.savefig(\"r_averages_\" + code + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(alphas, r_stds)\n",
    "    plt.title(\"r_stds_\" + code + \".png\")\n",
    "    plt.savefig(\"r_stds_\" + code + \".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(alphas, r_average_odds)\n",
    "    plt.plot(alphas, r_average_evens)\n",
    "    plt.title(\"r_even_ods_\" + code + \".png\")\n",
    "\n",
    "    plt.savefig(\"r_even_ods_\" + code + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    df= pd.DataFrame(data = {'alpha': alphas, \"r_averages\": r_averages, \"r_stds\": r_stds, 'r_average_odds':r_average_odds, 'r_average_evens':r_average_evens})\n",
    "    df.to_csv('r_averages_' + code + '.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410eeeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6908c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 10000\n",
    "# C = 100\n",
    "# gamma = 3\n",
    "# maxW = 100\n",
    "# t_averages = 10\n",
    "# T_stabilization = 10\n",
    "\n",
    "# figures_last_r(N, C, gamma, maxW, t_averages, T_stabilization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee2bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf823a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c93da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4cf973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac6bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fc6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  KuramotoModelVariation(KuramotoModel):\n",
    "        # DYNAMICS\n",
    "    # calculates one step of the Kuramoto function  z <-  2 z + (alpha/C) sum( sin(z_j - z))\n",
    "    def one_step(self): \n",
    "                    \n",
    "        V_cos = self.matrix.dot(self.cos_vector)    ##   timely steps\n",
    "        V_sin = self.matrix.dot(self.sin_vector)    ##\n",
    "        \n",
    "        self.th_vector = self.th_vector % (2 * np.pi)\n",
    "\n",
    "        self.th_vector = 2*(np.pi - np.abs(self.th_vector - np.pi))\n",
    "        self.th_vector += (self.alpha / self.C) * (V_sin * self.cos_vector - V_cos * self.sin_vector)\n",
    "        self.th_vector = self.th_vector % (2 * np.pi)\n",
    "        \n",
    "        self.update_r_theta()\n",
    "        self.r_historic.append(self.r)\n",
    "        self.theta_historic.append(self.theta)\n",
    "\n",
    "        return self.r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_Kuramoto_invGamma_file_Variation(N, C, gamma, maxW=10, filename = \"_\", verbose = False):\n",
    "\n",
    "    edg_lis = read_write_invGamma_graph(N, C, gamma ,maxW, filename = filename, verbose = verbose)\n",
    "        \n",
    "    if verbose: print(f\"Generating model from edge-list. N={N}, C={C}, g={gamma}\")    \n",
    "    mod =  KuramotoModelVariation(edg_lis)        \n",
    "\n",
    "    mod.gamma = gamma\n",
    "    mod.maxW = maxW\n",
    "    mod.initialization = 'init_Kuramoto_invGamma_file'\n",
    "    mod.graph_filename = 'invGamma_graph' + filename + f\"{N}_{C}_{gamma}_{maxW}.txt\"\n",
    "    \n",
    "    return mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78f726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "49c1bfd895b153fc6893037c7b56ceabc45de7a9afa10373955e246f0e309141"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
